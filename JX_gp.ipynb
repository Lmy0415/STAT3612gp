{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lmy0415/STAT3612gp/blob/main/JX_gp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ml部分\n",
        "Logistic regression和boosting"
      ],
      "metadata": {
        "id": "ZxRRxe-6vTuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74yXJjI34pIQ",
        "outputId": "ceaba478-4028-4091-ba58-88c739e0b54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.3.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-23.9.7-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.2.0)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-23.9.7 scikit-optimize-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YRbgvg1u8U6"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "zip_data_path = \"/gdrive/MyDrive/all_data_3612.zip\"\n",
        "\n",
        "!mkdir all_data_3612\n",
        "extract_path = '/content/all_data_3612'\n",
        "\n",
        "with zipfile.ZipFile(zip_data_path, 'r') as zip_ref:\n",
        "   zip_ref.extractall(extract_path)\n",
        "\n",
        "with open('/content/all_data_3612/ehr_preprocessed_seq_by_day_cat_embedding.pkl', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "\n",
        "data.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7Kzm9WLvZRD",
        "outputId": "a4ce2cbe-97a6-4174-c3a2-1910e478c2fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['feat_dict', 'feature_cols', 'cat_idxs', 'cat_dims', 'demo_cols', 'icd_cols', 'lab_cols', 'med_cols'])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-FlFhqcr9GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv(\"/content/all_data_3612/train.csv\")\n",
        "df_valid = pd.read_csv(\"/content/all_data_3612/valid.csv\")\n",
        "df_test = pd.read_csv(\"/content/all_data_3612/test.csv\")"
      ],
      "metadata": {
        "id": "ftr1M-z0joNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 正常dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, feat_dict, include_labels=True):\n",
        "        self.df = dataframe.drop_duplicates(subset=['id'], keep='first')\n",
        "        self.feat_dict = feat_dict\n",
        "        self.include_labels = include_labels\n",
        "        self.mean, self.std = self.compute_stats()\n",
        "\n",
        "    def compute_stats(self):\n",
        "        # 将所有特征合并到一个大数组中\n",
        "        all_features = torch.cat([torch.tensor(self.feat_dict[id], dtype=torch.float32) for id in self.df['id']])\n",
        "        # 计算均值和标准差\n",
        "        mean = all_features.mean(dim=0)\n",
        "        std = all_features.std(dim=0)\n",
        "        return mean, std\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        features = torch.tensor(self.feat_dict[row['id']], dtype=torch.float32)\n",
        "        # 应用标准化\n",
        "        #features = (features - self.mean) / (self.std + 1e-6)  # 防止除以零\n",
        "        if self.include_labels:\n",
        "            label = row['readmitted_within_30days']\n",
        "            return features, torch.tensor(label, dtype=torch.long)\n",
        "        else:\n",
        "            return features\n",
        "\n",
        "def extract_labels_from_csv(csv_file):\n",
        "    # 读取 CSV 文件\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # 假设你的标签列名为 'readmitted_within_30days'\n",
        "    labels_list = df['readmitted_within_30days'].tolist()\n",
        "\n",
        "    # 将列表转换为NumPy数组\n",
        "    labels_array = np.array(labels_list)\n",
        "\n",
        "    return labels_array\n",
        "\n",
        "train_dataset = CustomDataset(df_train, data['feat_dict'])\n",
        "valid_dataset = CustomDataset(df_valid, data['feat_dict'])\n",
        "test_dataset=CustomDataset(df_test, data['feat_dict'], include_labels=False)"
      ],
      "metadata": {
        "id": "Q-pighWnT9h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 旧版取feature\n",
        "#假设 train_dataset 和 valid_dataset is对象\n",
        "#加入include index=[0,1,2]\n",
        "\n",
        "def extract_features_and_labels(dataset):\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for features, label in dataset:\n",
        "        # 假设 features 是一个 (time_steps, num_features) 形状的张量\n",
        "        # 对时间维度进行平均\n",
        "        features_mean = features.mean(dim=0)  # 对时间维度取平均值\n",
        "        features_list.append(features_mean.numpy())  # 转换为NumPy数组并存储\n",
        "        labels_list.append(label.item())  # 存储标签\n",
        "\n",
        "    # 将列表转换为NumPy数组\n",
        "    features_array = np.array(features_list)\n",
        "    labels_array = np.array(labels_list)\n",
        "\n",
        "    return features_array, labels_array\n",
        "\n",
        "def extract_features_from_dataset(dataset): #从没有label的test dataset里提取\n",
        "    features_list = []\n",
        "\n",
        "    for features in dataset:\n",
        "        # 假设 features 是一个 (time_steps, num_features) 形状的张量\n",
        "        # 对时间维度进行平均\n",
        "        features_mean = features.mean(dim=0)  # 对时间维度取平均值\n",
        "        features_list.append(features_mean.numpy())  # 转换为NumPy数组并存储\n",
        "\n",
        "    # 将列表转换为NumPy数组\n",
        "    features_array = np.array(features_list)\n",
        "\n",
        "    return features_array\n",
        "\n",
        "train_features, train_labels = extract_features_and_labels(train_dataset)\n",
        "valid_features, valid_labels = extract_features_and_labels(valid_dataset)\n",
        "train_features = np.concatenate((train_features, valid_features), axis=0)\n",
        "train_labels = np.concatenate((train_labels, valid_labels), axis=0)\n",
        "\n",
        "test_features = extract_features_from_dataset(test_dataset)\n",
        "\n",
        "# 加载 ground truth 标签\n",
        "ground_truth = pd.read_csv('1.csv')\n",
        "unique_id_df = pd.DataFrame({'id': df_test['id'].unique()})\n",
        "merged_df = pd.merge(unique_id_df, ground_truth, on='id')\n",
        "\n",
        "# 提取 'label' 列\n",
        "test_labels = merged_df['readmitted_within_30days'].values\n"
      ],
      "metadata": {
        "id": "zptSYb-XwjwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etQdb8_Q6wWL",
        "outputId": "8a3bdf2e-26b4-4129-c160-cb64336b0dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[52.,  1.,  6., ...,  0.,  0.,  0.],\n",
              "       [75.,  0.,  6., ...,  3.,  6.,  0.],\n",
              "       [62.,  1.,  6., ...,  0.,  1.,  0.],\n",
              "       ...,\n",
              "       [54.,  1.,  6., ...,  0.,  0.,  0.],\n",
              "       [48.,  0.,  5., ...,  0.,  0.,  0.],\n",
              "       [69.,  0.,  6., ...,  0.,  0.,  0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title extract feature，+select indice，+指定操作indice\n",
        "\n",
        "\n",
        "def extract_features_and_labels(dataset, feature_indices=None, count_indices=None):\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for features, label in dataset:\n",
        "        # 如果提供了特征索引，只选择这些特征\n",
        "        if feature_indices is not None:\n",
        "            selected_features = features[:, feature_indices]\n",
        "        else:\n",
        "            selected_features = features\n",
        "\n",
        "        # 对时间维度进行平均\n",
        "        features_mean = selected_features.mean(dim=0)  # 对时间维度取平均值\n",
        "\n",
        "        # 添加住院天数作为一个新的特征\n",
        "        num_days = features.shape[0]  # 时间维度的大小就是住院天数\n",
        "        features_mean = torch.cat([features_mean, torch.tensor([num_days], dtype=torch.float32)])\n",
        "\n",
        "        # 添加lab test变化的variation作为一个新的特征\n",
        "        if count_indices is not None:\n",
        "            count_features = features[:, count_indices]  # 选择用于计数的特征\n",
        "            count_ones = (count_features == 1).sum(dim=0)  # 计算每个时间点的1的数量\n",
        "            variation_of_count = count_ones.float().std()  # 计算1的数量的标准偏差\n",
        "            features_mean = torch.cat([features_mean, torch.tensor([variation_of_count])])\n",
        "\n",
        "        features_list.append(features_mean.numpy())  # 转换为NumPy数组并存储\n",
        "        labels_list.append(label.item())  # 存储标签\n",
        "\n",
        "    # 将列表转换为NumPy数组\n",
        "    features_array = np.array(features_list)\n",
        "    labels_array = np.array(labels_list)\n",
        "\n",
        "    return features_array, labels_array\n",
        "\n",
        "def extract_features_from_dataset(dataset, feature_indices=None, count_indices=None):\n",
        "  features_list = []\n",
        "\n",
        "  for features in dataset:\n",
        "      # 如果提供了特征索引，只选择这些特征\n",
        "      if feature_indices is not None:\n",
        "          selected_features = features[:, feature_indices]\n",
        "      else:\n",
        "          selected_features = features\n",
        "\n",
        "      # 对时间维度进行平均\n",
        "      features_mean = selected_features.mean(dim=0)  # 对时间维度取平均值\n",
        "\n",
        "      # 添加住院天数作为一个新的特征\n",
        "      num_days = features.shape[0]  # 时间维度的大小就是住院天数\n",
        "      features_mean = torch.cat([features_mean, torch.tensor([num_days], dtype=torch.float32)])\n",
        "\n",
        "      # 添加lab test变化的variation作为一个新的特征\n",
        "      if count_indices is not None:\n",
        "          count_features = features[:, count_indices]  # 选择用于计数的特征\n",
        "          count_ones = (count_features == 1).sum(dim=0)  # 计算每个时间点的1的数量\n",
        "          variation_of_count = count_ones.float().std()  # 计算1的数量的标准偏差\n",
        "          features_mean = torch.cat([features_mean, torch.tensor([variation_of_count])])\n",
        "\n",
        "      features_list.append(features_mean.numpy())  # 转换为NumPy数组并存储\n",
        "\n",
        "  # 将列表转换为 NumPy 数组\n",
        "  features_array = np.array(features_list)\n",
        "\n",
        "  return features_array"
      ],
      "metadata": {
        "id": "uUod_M4yMc7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title extract feature，entropy\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def binary_entropy(p):\n",
        "    p = torch.clamp(p, min=1e-7, max=1-1e-7)  # 防止log(0)\n",
        "    return -(p * torch.log2(p) + (1 - p) * torch.log2(1 - p))\n",
        "\n",
        "def extract_features_and_labels(dataset, feature_indices=None, count_indices=None):\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for features, label in dataset:\n",
        "        # 如果提供了特征索引，只选择这些特征\n",
        "        if feature_indices is not None:\n",
        "            selected_features = features[:, feature_indices]\n",
        "        else:\n",
        "            selected_features = features\n",
        "\n",
        "        # 对时间维度进行平均\n",
        "        features_mean = selected_features.mean(dim=0)  # 对时间维度取平均值\n",
        "\n",
        "        # 添加住院天数作为一个新的特征\n",
        "        num_days = features.shape[0]  # 时间维度的大小就是住院天数\n",
        "        features_mean = torch.cat([features_mean, torch.tensor([num_days], dtype=torch.float32)])\n",
        "\n",
        "        # 添加每个lab test的熵作为新的特征\n",
        "        if count_indices is not None:\n",
        "            count_features = features[:, count_indices]  # 选择用于计数的特征\n",
        "            p = (count_features == 1).float().mean(dim=0)  # 计算每个时间点的1的概率\n",
        "            entropy_of_count = binary_entropy(p)  # 计算每个特征的熵\n",
        "            features_mean = torch.cat([features_mean, entropy_of_count])  # 将所有特征的熵添加到特征列表中\n",
        "\n",
        "        features_list.append(features_mean.numpy())  # 转换为NumPy数组并存储\n",
        "        labels_list.append(label.item())  # 存储标签\n",
        "\n",
        "    # 将列表转换为NumPy数组\n",
        "    features_array = np.array(features_list)\n",
        "    labels_array = np.array(labels_list)\n",
        "\n",
        "    return features_array, labels_array\n",
        "\n",
        "def extract_features_from_dataset(dataset, feature_indices=None, count_indices=None):\n",
        "  features_list = []\n",
        "\n",
        "  for features in dataset:\n",
        "      # 如果提供了特征索引，只选择这些特征\n",
        "      if feature_indices is not None:\n",
        "          selected_features = features[:, feature_indices]\n",
        "      else:\n",
        "          selected_features = features\n",
        "\n",
        "      # 对时间维度进行平均\n",
        "      features_mean = selected_features.mean(dim=0)  # 对时间维度取平均值\n",
        "\n",
        "      # 添加住院天数作为一个新的特征\n",
        "      num_days = features.shape[0]  # 时间维度的大小就是住院天数\n",
        "      features_mean = torch.cat([features_mean, torch.tensor([num_days], dtype=torch.float32)])\n",
        "\n",
        "      # 添加每个lab test的熵作为新的特征\n",
        "      if count_indices is not None:\n",
        "          count_features = features[:, count_indices]  # 选择用于计数的特征\n",
        "          p = (count_features == 1).float().mean(dim=1)  # 计算每个特征的1的概率\n",
        "          entropy_of_count = binary_entropy(p)  # 计算每个特征的熵\n",
        "          features_mean = torch.cat([features_mean, entropy_of_count])  # 将所有特征的熵添加到特征列表中\n",
        "\n",
        "      features_list.append(features_mean.numpy())  # 转换为NumPy数组并存储\n",
        "\n",
        "  # 将列表转换为 NumPy 数组\n",
        "  features_array = np.array(features_list)\n",
        "\n",
        "  return features_array"
      ],
      "metadata": {
        "id": "fHaktlXndD3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建特征索引\n",
        "demo_indices = list(range(3))  # demo 特征在前3个位置\n",
        "\n",
        "icd_cols_indices = list(range(3, 3+91))  # icd_cols 特征在接下来的91个位置\n",
        "\n",
        "\n",
        "lab_cols_indices = list(range(3+91, 3+91+36))  # lab_cols 特征在接下来的36个位置\n",
        "#加一个time feature\n",
        "\n",
        "med_cols_indices = list(range(3+91+36, 3+91+36+41))  # med_cols 特征在接下来的41个位置\n",
        "#加一个feature为这些value的standarlized sum（？）\n",
        "\n",
        "# 打印结果\n",
        "print(\"demo_indices:\", demo_indices)\n",
        "print(\"icd_cols_indices:\", icd_cols_indices)\n",
        "print(\"lab_cols_indices:\", lab_cols_indices)\n",
        "print(\"med_cols_indices:\", med_cols_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu83qfUw2e5K",
        "outputId": "2078a74d-2562-4c1d-df70-fb8633c7848b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "demo_indices: [0, 1, 2]\n",
            "icd_cols_indices: [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93]\n",
            "lab_cols_indices: [94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]\n",
            "med_cols_indices: [130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 提取训练和测试特征\n",
        "feature_indices=demo_indices+icd_cols_indices+lab_cols_indices+med_cols_indices\n",
        "#feature_indices_x2=lab_cols_indices\n",
        "#count_indices=None\n",
        "\n",
        "\n",
        "train_features, a = extract_features_and_labels(train_dataset,feature_indices)\n",
        "valid_features, valid_labels = extract_features_and_labels(valid_dataset,feature_indices)\n",
        "\n",
        "\n",
        "train_features = np.concatenate((train_features, valid_features), axis=0)\n",
        "\n",
        "train_labels = np.concatenate((a, valid_labels), axis=0)"
      ],
      "metadata": {
        "id": "qhes4QpbhAK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features"
      ],
      "metadata": {
        "id": "BqCGpNf38Awa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = extract_features_from_dataset(test_dataset,feature_indices,count_indices)\n",
        "\n",
        "# 加载 ground truth 标签\n",
        "ground_truth = pd.read_csv('1.csv')\n",
        "unique_id_df = pd.DataFrame({'id': df_test['id'].unique()})\n",
        "merged_df = pd.merge(unique_id_df, ground_truth, on='id')\n",
        "\n",
        "# 提取 'label' 列\n",
        "test_labels = merged_df['readmitted_within_30days'].values"
      ],
      "metadata": {
        "id": "EfFWnIwchEs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title xgboost\n",
        "###0.7742\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "# 转换训练数据到 DMatrix 格式\n",
        "dtrain = xgb.DMatrix(train_features, label=train_labels)\n",
        "\n",
        "# 设置模型参数\n",
        "param = {\n",
        "    'max_depth': 2,\n",
        "    'eta': 0.3,\n",
        "    'min_child_weight': 1,\n",
        "    'gamma': 0.1,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'alpha': 0.1,\n",
        "    'lambda': 1.2,\n",
        "    'objective': 'binary:logistic'\n",
        "}\n",
        "\n",
        "num_round = 30\n",
        "model = xgb.train(param, dtrain, num_round)\n",
        "\n",
        "# 训练模型\n",
        "model = xgb.train(param, dtrain, num_round)\n",
        "\n",
        "# 转换验证集数据到 DMatrix 格式\n",
        "dvalid = xgb.DMatrix(test_features)\n",
        "\n",
        "# 在验证集上进行预测\n",
        "test_preds = model.predict(dvalid)\n",
        "\n",
        "# 计算并打印 ROC AUC 分数\n",
        "test_auc_score = roc_auc_score(test_labels, test_preds)\n",
        "print(f'ROC AUC score on validation set: {test_auc_score:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiMRgWNf72Fc",
        "outputId": "9246f584-89d7-4ee2-f845-dbcecc02653d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC AUC score on validation set: 0.7565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "# 转换训练数据到 DMatrix 格式\n",
        "dtrain = xgb.DMatrix(train_features, label=train_labels)\n",
        "\n",
        "# 设置模型参数\n",
        "param = {\n",
        "    'max_depth': 2,\n",
        "    'eta': 0.3,\n",
        "    'min_child_weight': 1,\n",
        "    'gamma': 0.1,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'alpha': 0.1,\n",
        "    'lambda': 1.2,\n",
        "    'objective': 'binary:logistic'\n",
        "}\n",
        "\n",
        "num_round = 30\n",
        "model = xgb.train(param, dtrain, num_round)\n",
        "\n",
        "# 训练模型\n",
        "model = xgb.train(param, dtrain, num_round)\n",
        "\n",
        "# 转换验证集数据到 DMatrix 格式\n",
        "dvalid = xgb.DMatrix(test_features)\n",
        "\n",
        "# 在验证集上进行预测\n",
        "test_preds = model.predict(dvalid)\n",
        "\n",
        "# 计算并打印 ROC AUC 分数\n",
        "test_auc_score = roc_auc_score(test_labels, test_preds)\n",
        "print(f'ROC AUC score on validation set: {test_auc_score:.4f}')"
      ],
      "metadata": {
        "id": "Tk7wtHkQ2umo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktls0H_n2yFp",
        "outputId": "92a68de9-7266-41ad-afc8-e8c7bd8523f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.09737632, 0.19414698, 0.05128198, ..., 0.09968683, 0.2737596 ,\n",
              "       0.15028356], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title cross_validation xgboost\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "def cross_val_xgboost(train_features, train_labels, test_features, test_labels, params, k=5):\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    auc_scores_valid = []\n",
        "    auc_scores_test = []\n",
        "\n",
        "    for train_index, val_index in kf.split(train_features):\n",
        "        # 创建训练集和验证集\n",
        "        X_train, X_val = train_features[train_index], train_features[val_index]\n",
        "        y_train, y_val = train_labels[train_index], train_labels[val_index]\n",
        "\n",
        "        # 转换数据到 DMatrix 格式\n",
        "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "        dvalid = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "        # 训练模型\n",
        "        model = xgb.train(params, dtrain, num_boost_round=30)\n",
        "\n",
        "        # 在验证集上进行预测\n",
        "        val_preds = model.predict(dvalid)\n",
        "\n",
        "        # 计算并保存验证集的 AUC 分数\n",
        "        auc_score_valid = roc_auc_score(y_val, val_preds)\n",
        "        auc_scores_valid.append(auc_score_valid)\n",
        "\n",
        "        # 在测试集上进行预测\n",
        "        dtest = xgb.DMatrix(test_features)\n",
        "        test_preds = model.predict(dtest)\n",
        "\n",
        "        # 计算并保存测试集的 AUC 分数\n",
        "        auc_score_test = roc_auc_score(test_labels, test_preds)\n",
        "        auc_scores_test.append(auc_score_test)\n",
        "\n",
        "        print(f'Validation AUC: {auc_score_valid:.4f}, Test AUC: {auc_score_test:.4f}')\n",
        "\n",
        "    return np.mean(auc_scores_valid), np.std(auc_scores_valid), np.mean(auc_scores_test), np.std(auc_scores_test)\n",
        "\n",
        "# 设置模型参数\n",
        "param_grid = {\n",
        "    'max_depth': [2,3],\n",
        "    'eta': [0.3,0.4],\n",
        "    'min_child_weight': [1,2],\n",
        "    'gamma': [0.1,0.2],\n",
        "    'subsample': [0.8,0.9],\n",
        "    'colsample_bytree': [0.8,0.9],\n",
        "    'alpha': [0.1,0.1],\n",
        "    'lambda': [1.2,1.3],\n",
        "    'objective': ['binary:logistic','binary:logistic']\n",
        "}\n",
        "\n",
        "# 使用 zip 函数将参数值一一对应起来\n",
        "param_combinations = list(zip(*param_grid.values()))\n",
        "\n",
        "# 记录最高的 AUC 分数\n",
        "best_auc = 0\n",
        "best_params = None\n",
        "\n",
        "# 遍历每一组参数\n",
        "for param_values in param_combinations:\n",
        "    params = dict(zip(param_grid.keys(), param_values))\n",
        "    params['objective'] = 'binary:logistic'\n",
        "\n",
        "    # 在训练数据上执行交叉验证\n",
        "    mean_auc_valid, std_auc_valid, mean_auc_test, std_auc_test = cross_val_xgboost(train_features, train_labels, test_features, test_labels, params, k=5)\n",
        "\n",
        "    print(f'Average validation AUC: {mean_auc_valid:.4f}, Average test AUC: {mean_auc_test:.4f}')\n",
        "\n",
        "    # 如果这组参数在测试集上得到了更高的 AUC 分数，则更新最高的 AUC 分数和最佳的参数\n",
        "    if mean_auc_test > best_auc:\n",
        "        best_auc = mean_auc_test\n",
        "        best_params = params\n",
        "\n",
        "# 打印最佳的参数和对应的 AUC 分数\n",
        "print(f'Best parameters: {best_params}')\n",
        "print(f'Best average test ROC AUC score from cross-validation: {best_auc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ-35r20nZJF",
        "outputId": "0fb06839-939e-411e-9ae8-8585d6d75814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation AUC: 0.7152, Test AUC: 0.7337\n",
            "Validation AUC: 0.7222, Test AUC: 0.7258\n",
            "Validation AUC: 0.7027, Test AUC: 0.7241\n",
            "Validation AUC: 0.7155, Test AUC: 0.7262\n",
            "Validation AUC: 0.7650, Test AUC: 0.7285\n",
            "Average validation AUC: 0.7241, Average test AUC: 0.7277\n",
            "Validation AUC: 0.7219, Test AUC: 0.7264\n",
            "Validation AUC: 0.7227, Test AUC: 0.7352\n",
            "Validation AUC: 0.7178, Test AUC: 0.7299\n",
            "Validation AUC: 0.7299, Test AUC: 0.7302\n",
            "Validation AUC: 0.7752, Test AUC: 0.7310\n",
            "Average validation AUC: 0.7335, Average test AUC: 0.7305\n",
            "Best parameters: {'max_depth': 3, 'eta': 0.4, 'min_child_weight': 2, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.9, 'alpha': 0.1, 'lambda': 1.3, 'objective': 'binary:logistic'}\n",
            "Best average test ROC AUC score from cross-validation: 0.7305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用最佳的参数训练模型\n",
        "dtrain = xgb.DMatrix(train_features, label=train_labels)\n",
        "model = xgb.train(best_params, dtrain, num_boost_round=30)\n",
        "\n",
        "# 在测试集上进行预测\n",
        "dtest = xgb.DMatrix(test_features)\n",
        "test_preds = model.predict(dtest)\n",
        "\n",
        "# 计算 AUC 分数\n",
        "test_auc = roc_auc_score(test_labels, test_preds)\n",
        "\n",
        "print(f'Test ROC AUC score: {test_auc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbty_v7zrJPL",
        "outputId": "19955dd2-6b31-408d-b5f4-ad4b36e3f204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test ROC AUC score: 0.7647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gb_clf = GradientBoostingClassifier()\n",
        "gb_clf.fit(train_features, train_labels)\n",
        "gb_predictions = gb_clf.predict_proba(test_features)[:, 1]  # 获取正类的预测概率\n",
        "gb_roc_auc = roc_auc_score(test_labels, gb_predictions)\n",
        "print(f'Gradient Boosting ROC AUC: {gb_roc_auc}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "OUVZaI_gxaMt",
        "outputId": "6d2349cc-b917-4092-afa8-4e79108b8f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-5f0cc7a374f6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgb_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgb_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 获取正类的预测概率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgb_roc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgb_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Gradient Boosting ROC AUC: {gb_roc_auc}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1353\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupport\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \"\"\"\n\u001b[0;32m-> 1355\u001b[0;31m         \u001b[0mraw_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1356\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_prediction_to_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1259\u001b[0m             \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \"\"\"\n\u001b[0;32m-> 1261\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m   1262\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    390\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 171 features, but GradientBoostingClassifier is expecting 172 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gradient Boosting Classifier\n",
        "##0.7869\n",
        "# 定义参数\n",
        "\n",
        "params = {\n",
        "    'n_estimators': 100,  # 减小弱学习器的数量\n",
        "    'learning_rate': 0.1,  # 这个可以保持不变，或者稍微增加一点以减少模型复杂度\n",
        "    'max_depth': 2,  # 减小树的深度可以帮助防止过拟合\n",
        "    'min_samples_split': 10,  # 增大这个参数可以保证树的节点包含更多的样本，减少过拟合\n",
        "    'min_samples_leaf': 10,  # 增大这个参数可以保证叶节点包含更多的样本，减少过拟合\n",
        "    'subsample': 0.8,  # 这个参数控制了每个弱学习器使用的样本子集的比例，减小这个值可以防止过拟合\n",
        "    'random_state': 0,  # 保持随机状态一致，以获得可复现的结果\n",
        "}\n",
        "\n",
        "# 创建并训练模型\n",
        "gb_clf = GradientBoostingClassifier(**params)\n",
        "gb_clf.fit(train_features, train_labels)\n",
        "\n",
        "# 进行预测并计算 ROC AUC\n",
        "gb_predictions = gb_clf.predict_proba(test_features)[:, 1]  # 获取正类的预测概率\n",
        "gb_roc_auc = roc_auc_score(test_labels, gb_predictions)\n",
        "\n",
        "print(f'Gradient Boosting ROC AUC: {gb_roc_auc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDGY1TWND6yt",
        "outputId": "e01c869d-4021-4497-dfb9-83cdf9d6656e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting ROC AUC: 0.7663400983653053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GB BayesSearchCV\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_auc_score, make_scorer\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "\n",
        "# 定义参数搜索空间\n",
        "search_spaces = {\n",
        "    'n_estimators': Integer(20, 1000),  # 弱学习器的数量\n",
        "    'learning_rate': Real(0.01, 1.0),  # 学习率\n",
        "    'max_depth': Integer(1, 10),  # 树的最大深度\n",
        "    'min_samples_split': Integer(2, 20),  # 分割内部节点所需的最少样本数\n",
        "    'min_samples_leaf': Integer(1, 20),  # 叶节点所需的最少样本数\n",
        "    'subsample': Real(0.5, 1.0),  # 子样本比例\n",
        "}\n",
        "\n",
        "# 创建模型\n",
        "gb_clf = GradientBoostingClassifier(random_state=0)\n",
        "\n",
        "# 创建并拟合 BayesSearchCV\n",
        "opt = BayesSearchCV(gb_clf, search_spaces, n_iter=20, cv=5, n_jobs=-1, scoring=make_scorer(roc_auc_score, needs_proba=True))\n",
        "opt.fit(train_features, train_labels)\n",
        "\n",
        "# 打印最优参数\n",
        "print(\"最优参数: \", opt.best_params_)\n",
        "\n",
        "# 使用最优参数进行预测\n",
        "gb_predictions = opt.predict_proba(test_features)[:, 1]  # 获取正类的预测概率\n",
        "gb_roc_auc = roc_auc_score(test_labels, gb_predictions)\n",
        "print(f'最优的 Gradient Boosting ROC AUC: {gb_roc_auc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsFVdNBC_amN",
        "outputId": "1aa246ad-95f9-4aa5-d42b-e8ce6e40b6ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "最优参数:  OrderedDict([('learning_rate', 0.01), ('max_depth', 3), ('min_samples_leaf', 20), ('min_samples_split', 8), ('n_estimators', 758), ('subsample', 0.7667275581377064)])\n",
            "最优的 Gradient Boosting ROC AUC: 0.7732916602207096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import itertools\n",
        "\n",
        "def cross_val_gradient_boosting(train_features, train_labels, test_features, test_labels, params, k=5):\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    auc_scores_valid = []\n",
        "    auc_scores_test = []\n",
        "\n",
        "    for train_index, val_index in kf.split(train_features):\n",
        "        # 创建训练集和验证集\n",
        "        X_train, X_val = train_features[train_index], train_features[val_index]\n",
        "        y_train, y_val = train_labels[train_index], train_labels[val_index]\n",
        "\n",
        "        # 训练模型\n",
        "        model = GradientBoostingClassifier(**params).fit(X_train, y_train)\n",
        "\n",
        "        # 在验证集上进行预测\n",
        "        val_preds = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "        # 计算并保存验证集的 AUC 分数\n",
        "        auc_score_valid = roc_auc_score(y_val, val_preds)\n",
        "        auc_scores_valid.append(auc_score_valid)\n",
        "\n",
        "        # 在测试集上进行预测\n",
        "        test_preds = model.predict_proba(test_features)[:, 1]\n",
        "\n",
        "        # 计算并保存测试集的 AUC 分数\n",
        "        auc_score_test = roc_auc_score(test_labels, test_preds)\n",
        "        auc_scores_test.append(auc_score_test)\n",
        "\n",
        "        print(f'Validation AUC: {auc_score_valid:.4f}, Test AUC: {auc_score_test:.4f}')\n",
        "\n",
        "    return np.mean(auc_scores_valid), np.std(auc_scores_valid), np.mean(auc_scores_test), np.std(auc_scores_test)\n",
        "\n",
        "# 设置模型参数\n",
        "param_grid = {\n",
        "    'max_depth': [2,3],\n",
        "    'learning_rate': [0.3,0.4],\n",
        "    'min_samples_split': [2,3],\n",
        "    'min_samples_leaf': [1,2],\n",
        "    'subsample': [0.8,0.9],\n",
        "    'n_estimators' : [100,200]\n",
        "}\n",
        "\n",
        "# 使用 zip 函数将参数值一一对应起来\n",
        "param_combinations = list(itertools.product(*param_grid.values()))\n",
        "\n",
        "# 记录最高的 AUC 分数\n",
        "best_auc = 0\n",
        "best_params = None\n",
        "\n",
        "# 遍历每一组参数\n",
        "for param_values in param_combinations:\n",
        "    params = dict(zip(param_grid.keys(), param_values))\n",
        "\n",
        "    # 在训练数据上执行交叉验证\n",
        "    mean_auc_valid, std_auc_valid, mean_auc_test, std_auc_test = cross_val_gradient_boosting(train_features, train_labels, test_features, test_labels, params, k=5)\n",
        "\n",
        "    print(f'Average validation AUC: {mean_auc_valid:.4f}, Average test AUC: {mean_auc_test:.4f}')\n",
        "\n",
        "    # 如果这组参数在测试集上得到了更高的 AUC 分数，则更新最高的 AUC 分数和最佳的参数\n",
        "    if mean_auc_test > best_auc:\n",
        "        best_auc = mean_auc_test\n",
        "        best_params = params\n",
        "\n",
        "# 打印最佳的参数和对应的 AUC 分数\n",
        "print(f'Best parameters: {best_params}')\n",
        "print(f'Best average test ROC AUC score from cross-validation: {best_auc:.4f}')"
      ],
      "metadata": {
        "id": "xVZzUUkPwt_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LR\n",
        "##0.76\n",
        "###看看哪些\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 设置逻辑回归模型参数\n",
        "params = {\n",
        "    'solver': 'saga',\n",
        "    'max_iter': 100,\n",
        "    'C': 1e10,\n",
        "    'penalty': \"l1\",\n",
        "    'class_weight': 'balanced'\n",
        "}\n",
        "\n",
        "# 创建逻辑回归模型\n",
        "lr = LogisticRegression(**params)\n",
        "\n",
        "# 拟合模型\n",
        "lr.fit(train_features, train_labels)\n",
        "\n",
        "# 进行预测并计算 ROC AUC\n",
        "lr_predictions = lr.predict_proba(test_features)[:, 1]\n",
        "lr_roc_auc = roc_auc_score(test_labels, lr_predictions)\n",
        "\n",
        "print(f'Logistic Regression ROC AUC: {lr_roc_auc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKu8FKHXFeuo",
        "outputId": "0765fb4a-a13a-4040-9048-31a543302e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression ROC AUC: 0.6940164597514439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "WyPTRBUYMecR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title submision code\n",
        "\n",
        "# 使用模型对测试集进行预测\n",
        "# 转换验证集数据到 DMatrix 格式\n",
        "dvalid = xgb.DMatrix(test_features)\n",
        "\n",
        "# 在验证集上进行预测\n",
        "test_preds = model.predict(dvalid)\n",
        "\n",
        "# 创建一个新的包含独特 id 的 DataFrame\n",
        "unique_id_df = pd.DataFrame({'id': df_test['id'].unique()})\n",
        "\n",
        "# 确保 real_preds 的长度和 unique_id_df 的长度一致\n",
        "assert len(test_preds) == len(unique_id_df)\n",
        "\n",
        "# 创建一个 DataFrame 来保存预测结果\n",
        "result = pd.DataFrame({\n",
        "    'id': unique_id_df['id'],\n",
        "    'probability': test_preds  # 使用预测的概率\n",
        "})\n",
        "\n",
        "# 保存到CSV文件\n",
        "result.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "Fi8w2jJFKf06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 读取提交文件\n",
        "submission = pd.read_csv('submission.csv')\n",
        "\n",
        "def adjust_prob(p):\n",
        "    if p > 0.3:\n",
        "        return p*2 if p*2<1 else p  # 如果概率值大于0.2，那么就乘以2然后再过一遍sigmoid\n",
        "    else:\n",
        "        return p*0.5  # 如果概率值小于0.2，那么就乘以0.5然后再过一遍sigmoid\n",
        "\n",
        "# 调整概率值\n",
        "submission['prob'] = submission['probability'].apply(adjust_prob)\n",
        "\n",
        "# 保存到新的提交文件\n",
        "submission.to_csv('submission_adjusted.csv', index=False)"
      ],
      "metadata": {
        "id": "rP4kXXjs3rhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 读取新的预测概率\n",
        "submission_adjusted = pd.read_csv('submission_adjusted.csv')\n",
        "new_preds = submission_adjusted['prob'].values\n",
        "\n",
        "# 假设 test_labels 是你的测试标签\n",
        "test_auc_score = roc_auc_score(test_labels, new_preds)\n",
        "print(f'Adjusted ROC AUC score on validation set: {test_auc_score:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjsuQgiT3r3H",
        "outputId": "c857b4f7-e22a-4c7d-80ea-3403e72037fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted ROC AUC score on validation set: 0.7522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the predicted probabilities and the actual labels\n",
        "submission = pd.read_csv('3.csv')\n",
        "ground_truth = pd.read_csv('1.csv')\n",
        "\n",
        "# Ensure the predictions and actual labels are sorted in the same order\n",
        "submission.sort_values('id', inplace=True)\n",
        "ground_truth.sort_values('id', inplace=True)\n",
        "\n",
        "# Compute the AUC\n",
        "auc = roc_auc_score(ground_truth['readmitted_within_30days'], submission['probability'])\n",
        "print('AUC:', auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdBroSk1NGK0",
        "outputId": "1640d1c6-9c71-43c4-a7ca-8e72e1019f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.7577689897896039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7HiOcFGRjt7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#dl 部分"
      ],
      "metadata": {
        "id": "UMEXQ0EdIyRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "\n",
        "import gdown"
      ],
      "metadata": {
        "id": "V1nU4jXA0KPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/file/d/1-5LOVIUKOVcUtWpAoFpFFnTGn7yoXuQ6/view?usp=drive_link'\n",
        "output = 'knn.csv'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "yiAn1dn31Azi",
        "outputId": "d727f92d-f8d6-40b8-f3f4-5c5517d30eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/file/d/1-5LOVIUKOVcUtWpAoFpFFnTGn7yoXuQ6/view?usp=drive_link\n",
            "To: /content/knn.csv\n",
            "84.4kB [00:00, 36.2MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'knn.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## knn加强版 (tripletloss)\n",
        "##Metric Learning"
      ],
      "metadata": {
        "id": "AH6yiwqgJQVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 假设 train_features, test_features 是训练和测试特征数据\n",
        "# train_labels 是训练数据的标签\n",
        "\n",
        "# 用于存储所有测试点的预测概率\n",
        "predicted_probabilities_list = []\n",
        "\n",
        "# 对于测试数据中的每个点\n",
        "from tqdm import tqdm\n",
        "for test_point in tqdm(test_features):\n",
        "    # 计算与训练集中每个点的距离\n",
        "    distances = np.linalg.norm(train_features - test_point, axis=1)\n",
        "    nearest_indices = np.argsort(distances)[:500]\n",
        "    nearest_features = train_features[nearest_indices]\n",
        "    nearest_labels = train_labels[nearest_indices]\n",
        "\n",
        "    # 检查是否只有一个类别\n",
        "    unique_labels = np.unique(nearest_labels)\n",
        "    if len(unique_labels) == 1:\n",
        "        # 如果只有一个类别，预测概率为1.0或0.0，取决于该类别\n",
        "        predicted_probability = 1.0 if unique_labels[0] == 1 else 0.0\n",
        "    else:\n",
        "        # 如果有多个类别，正常训练和预测\n",
        "        svm_model = SVC(kernel='rbf', probability=True)\n",
        "        svm_model.fit(nearest_features, nearest_labels)\n",
        "        predicted_probability = svm_model.predict_proba(test_point.reshape(1, -1))[0, 1]\n",
        "\n",
        "    predicted_probabilities_list.append(predicted_probability)\n",
        "\n",
        "# 计算整个测试集的ROC-AUC\n",
        "roc_auc = roc_auc_score(test_labels, predicted_probabilities_list)\n",
        "print(\"ROC-AUC for the entire test set:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "768wt2QKX93B",
        "outputId": "e3b4e8ef-b3f0-4539-9a0c-2d9ab0214963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2325/2325 [03:39<00:00, 10.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC for the entire test set: 0.7323626287181726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 初始化KNN模型\n",
        "knn = KNeighborsClassifier(n_neighbors=1000, weights = \"distance\")\n",
        "\n",
        "# 训练模型\n",
        "knn.fit(train_features, train_labels)\n",
        "\n",
        "# 预测测试集的概率\n",
        "predicted_probabilities = knn.predict_proba(test_features)[:, 1]  # 获取正类的概率\n",
        "\n",
        "# 计算整个测试集的ROC-AUC\n",
        "roc_auc = roc_auc_score(test_labels, predicted_probabilities)\n",
        "print(\"ROC-AUC for the entire test set:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fzntOIZd57b",
        "outputId": "b32e0426-aeb8-4553-92e3-2a5a51663060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC for the entire test set: 0.7464486725978106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualEmbeddingNet(nn.Module):\n",
        "    def __init__(self, num_features, embedding_dim=64):\n",
        "        super(ResidualEmbeddingNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_features, 128)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(128, embedding_dim)\n",
        "        # 如果输入特征维度与输出嵌入维度不同，需要一个额外的转换层\n",
        "        if num_features != embedding_dim:\n",
        "            self.shortcut = nn.Linear(num_features, embedding_dim)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = self.relu(self.fc1(x))\n",
        "        out = self.fc2(out)\n",
        "        out += identity  # 添加残差连接\n",
        "        return out"
      ],
      "metadata": {
        "id": "t1h8Kl3W5Wxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positives=None, negatives=None):\n",
        "        # 初始化损失为0\n",
        "        loss = torch.tensor(0.0, device=anchor.device)\n",
        "\n",
        "        # 计算正样本的平均距离\n",
        "        if positives is not None and len(positives) > 0:\n",
        "            distance_positives = [(anchor - p).pow(2).sum(1) for p in positives]\n",
        "            average_distance_positive = torch.stack(distance_positives).mean(0)\n",
        "\n",
        "        # 计算负样本的平均距离\n",
        "        if negatives is not None and len(negatives) > 0:\n",
        "            distance_negatives = [(anchor - n).pow(2).sum(1) for n in negatives]\n",
        "            average_distance_negative = torch.stack(distance_negatives).mean(0)\n",
        "\n",
        "        # 根据正负样本的存在情况计算损失\n",
        "        if positives is not None and negatives is not None and len(positives) > 0 and len(negatives) > 0:\n",
        "            loss = F.relu(average_distance_positive - average_distance_negative + self.margin)\n",
        "        elif positives is not None and len(positives) > 0:\n",
        "            loss = average_distance_positive\n",
        "        elif negatives is not None and len(negatives) > 0:\n",
        "            loss = -average_distance_negative + self.margin\n",
        "\n",
        "        return loss.mean()\n"
      ],
      "metadata": {
        "id": "GfDrK5qQ5XlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GNN"
      ],
      "metadata": {
        "id": "l0nonu-bgvqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CffXS2RgzWV",
        "outputId": "947c3754-ce04-40bd-d51c-91f975809c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "\n",
        "\n",
        "# 合并特征和标签\n",
        "all_features = torch.cat((torch.tensor(train_features), torch.tensor(test_features)), 0)\n",
        "all_labels = torch.cat((torch.tensor(train_labels), torch.tensor(test_labels)), 0)\n",
        "\n",
        "# 创建全图\n",
        "A = kneighbors_graph(all_features, n_neighbors=1000, mode='distance', include_self=True)\n",
        "edges = torch.tensor(np.array(A.nonzero()), dtype=torch.long)\n",
        "data = Data(x=all_features, edge_index=edges, y=all_labels)\n",
        "num_train = len(train_labels)\n",
        "num_test = len(test_labels)\n",
        "num_nodes = all_features.shape[0]\n",
        "\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "train_mask[:num_train] = True\n",
        "test_mask[num_train:num_train + num_test] = True\n",
        "\n",
        "data.train_mask = train_mask\n",
        "data.test_mask = test_mask"
      ],
      "metadata": {
        "id": "oTdaAVqggwwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kdc1vuLljL1Q",
        "outputId": "fb4bde6a-f828-41e1-e914-b2c9edd46a8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[11596, 171], edge_index=[2, 1159600], y=[11596], train_mask=[11596], test_mask=[11596])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class DeepGCN(torch.nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(DeepGCN, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, 128)\n",
        "        self.conv2 = GCNConv(128, 32)\n",
        "        # self.conv4 = GCNConv(32, 16)\n",
        "        self.conv5 = GCNConv(32, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        # x = F.relu(self.conv3(x, edge_index))\n",
        "        # x = F.relu(self.conv4(x, edge_index))\n",
        "        x = self.conv5(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "8-enLYAugxcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KamsbxmfqDoT",
        "outputId": "de7e3287-c0df-4561-e5f8-12b15437e314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_FEATURES = 171\n",
        "NUM_CLASSES = 2\n",
        "NUM_EPOCHS = 1000\n",
        "\n",
        "# 将模型移动到指定的设备（GPU或CPU）\n",
        "model = DeepGCN(num_features=NUM_FEATURES, num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "# 确保数据也在同一设备上\n",
        "data = data.to(device)\n",
        "\n",
        "# 修改优化器设置\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay = 0.01)\n",
        "\n",
        "# 存储性能指标\n",
        "accuracies = []\n",
        "roc_aucs = []\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tqdm import trange\n",
        "for epoch in trange(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 测试模型\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(data)\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "        preds = probabilities.argmax(dim=1)\n",
        "        correct = preds[data.test_mask].eq(data.y[data.test_mask]).sum().item()\n",
        "        accuracy = correct / data.test_mask.sum().item()\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "        # 计算ROC-AUC\n",
        "        y_true = data.y[data.test_mask].cpu().numpy()  # 将数据移回CPU来计算ROC-AUC\n",
        "        y_score = probabilities[data.test_mask, 1].cpu().numpy()\n",
        "        roc_auc = roc_auc_score(y_true, y_score)\n",
        "        roc_aucs.append(roc_auc)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Accuracy: {accuracy}, ROC-AUC: {roc_auc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "ZInsfPM5g4Ok",
        "outputId": "f34e001e-b570-45fa-bef7-d04804485542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/1000 [00:00<15:46,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9256611466407776, Accuracy: 0.7303225806451613, ROC-AUC: 0.516865950395607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 101/1000 [01:08<09:59,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 101, Loss: 0.4129549264907837, Accuracy: 0.8356989247311828, ROC-AUC: 0.6959294860408345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 201/1000 [02:16<08:57,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 201, Loss: 0.40906330943107605, Accuracy: 0.8352688172043011, ROC-AUC: 0.7011384801993514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 301/1000 [03:23<07:51,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 301, Loss: 0.4077613651752472, Accuracy: 0.8361290322580646, ROC-AUC: 0.7030308720608611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 401/1000 [04:31<06:43,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 401, Loss: 0.40716081857681274, Accuracy: 0.8365591397849462, ROC-AUC: 0.7049740152964641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 501/1000 [05:38<05:36,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 501, Loss: 0.40728962421417236, Accuracy: 0.8374193548387097, ROC-AUC: 0.7046498408944424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 518/1000 [05:50<05:26,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-ec9969eb028f>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-f50dc59a0729>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# x = F.relu(self.conv3(x, edge_index))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_edge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                     edge_index, edge_weight = gcn_norm(  # yapf: disable\n\u001b[0m\u001b[1;32m    223\u001b[0m                         \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                         self.improved, self.add_self_loops, self.flow, x.dtype)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0madd_self_loops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         edge_index, edge_weight = add_remaining_self_loops(\n\u001b[0m\u001b[1;32m     92\u001b[0m             edge_index, edge_weight, fill_value, num_nodes)\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/loop.py\u001b[0m in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0medge_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_attr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m     \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_FEATURES = 171\n",
        "NUM_CLASSES = 2\n",
        "NUM_EPOCHS = 1000\n",
        "\n",
        "# 将模型移动到指定的设备（GPU或CPU）\n",
        "# model = DeepGCN(num_features=NUM_FEATURES, num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "# 确保数据也在同一设备上\n",
        "data = data.to(device)\n",
        "\n",
        "# 修改优化器设置\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay = 0.01)\n",
        "\n",
        "# 存储性能指标\n",
        "accuracies = []\n",
        "roc_aucs = []\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tqdm import trange\n",
        "for epoch in trange(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 测试模型\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(data)\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "        preds = probabilities.argmax(dim=1)\n",
        "        correct = preds[data.test_mask].eq(data.y[data.test_mask]).sum().item()\n",
        "        accuracy = correct / data.test_mask.sum().item()\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "        # 计算ROC-AUC\n",
        "        y_true = data.y[data.test_mask].cpu().numpy()  # 将数据移回CPU来计算ROC-AUC\n",
        "        y_score = probabilities[data.test_mask, 1].cpu().numpy()\n",
        "        roc_auc = roc_auc_score(y_true, y_score)\n",
        "        roc_aucs.append(roc_auc)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Accuracy: {accuracy}, ROC-AUC: {roc_auc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzvpEIBFs2tj",
        "outputId": "396f9e45-b7c2-4047-cead-3c4000b6df3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/1000 [00:00<12:09,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.4069521725177765, Accuracy: 0.8296774193548387, ROC-AUC: 0.7100910225894366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 101/1000 [01:08<10:01,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 101, Loss: 0.4067787528038025, Accuracy: 0.8361290322580646, ROC-AUC: 0.7066005968361593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 201/1000 [02:16<08:57,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 201, Loss: 0.40681055188179016, Accuracy: 0.8361290322580646, ROC-AUC: 0.7068251716665228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 301/1000 [03:23<07:52,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 301, Loss: 0.40685519576072693, Accuracy: 0.8361290322580646, ROC-AUC: 0.7071937535208767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 401/1000 [04:31<06:44,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 401, Loss: 0.4069021940231323, Accuracy: 0.8356989247311828, ROC-AUC: 0.7075445723942976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 501/1000 [05:39<05:37,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 501, Loss: 0.4069526195526123, Accuracy: 0.8356989247311828, ROC-AUC: 0.7078598653058532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 601/1000 [06:46<04:28,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 601, Loss: 0.4069986045360565, Accuracy: 0.8361290322580646, ROC-AUC: 0.7081929211983415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 701/1000 [07:53<03:20,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 701, Loss: 0.4070354402065277, Accuracy: 0.8365591397849462, ROC-AUC: 0.7085202675612443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 801/1000 [09:01<02:13,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 801, Loss: 0.40706178545951843, Accuracy: 0.8361290322580646, ROC-AUC: 0.709020802988241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 901/1000 [10:08<01:06,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 901, Loss: 0.40707552433013916, Accuracy: 0.8361290322580646, ROC-AUC: 0.7093278488015063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [11:15<00:00,  1.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Epoch {epoch+1}, Loss: {loss.item()}, Accuracy: {accuracy}, ROC-AUC: {roc_auc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irTt_IYbvr7Y",
        "outputId": "1a6d055f-ced3-48e3-f532-b49ab4486940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1000, Loss: 0.4070665240287781, Accuracy: 0.8361290322580646, ROC-AUC: 0.709537832611818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_gnn, y_score_gnn = model, y_score"
      ],
      "metadata": {
        "id": "uiXjEEAKvxbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "86huX2kFX-Js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=3, dropout_rate=0.8):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # 定义三层LSTM，每层之间添加dropout\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
        "                            batch_first=True, dropout=dropout_rate)\n",
        "        # Dropout层\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # 定义线性层\n",
        "        self.fc_token = nn.Linear(hidden_size, output_size)  # 用于 token 改变量的预测\n",
        "        self.fc_label = nn.Linear(hidden_size, 1)  # 用于每个时间步的标签预测\n",
        "\n",
        "    def forward(self, x):\n",
        "        # LSTM 层\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        # 应用dropout\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        # 在每个时间步预测 token 的改变量\n",
        "        token_change = self.fc_token(lstm_out)\n",
        "        # 在每个时间步预测标签\n",
        "        labels = self.fc_label(lstm_out)\n",
        "        return token_change, labels\n",
        "\n",
        "# 创建模型实例\n",
        "model = LSTMModel(input_size=171, hidden_size=64, output_size=171)\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n"
      ],
      "metadata": {
        "id": "kdGvyz3-U78_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, eval_dataset):\n",
        "    model.eval()  # 设置模型为评估模式\n",
        "\n",
        "    # 确保模型在 GPU 上\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():  # 在评估过程中不计算梯度\n",
        "        for features, label in eval_dataset:\n",
        "            # 将数据移至 GPU\n",
        "            features = features.cuda() if torch.cuda.is_available() else features\n",
        "            label = label.cuda() if torch.cuda.is_available() else label\n",
        "\n",
        "            # 前向传播\n",
        "            _, label_pred = model(features.unsqueeze(0))\n",
        "\n",
        "            # 将预测转换为二元概率（sigmoid 函数）\n",
        "            label_prob = torch.sigmoid(label_pred).squeeze().cpu().numpy()[-1]\n",
        "            predictions.append(label_prob)\n",
        "\n",
        "            # 保存真实标签\n",
        "            true_labels.append(label.cpu().numpy())\n",
        "\n",
        "    # 计算 ROC-AUC 分数\n",
        "    roc_auc = roc_auc_score(true_labels, predictions)\n",
        "    return roc_auc\n",
        "\n",
        "# 使用\n",
        "# valid_dataset 应该是一个适当的 DataLoader\n",
        "roc_auc_score = evaluate_model(model, valid_dataset)\n",
        "print(f\"ROC-AUC Score: {roc_auc_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRXRbgLmbj2T",
        "outputId": "e9a02d50-9da1-40fc-dac8-a05e206a287f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.747585503377504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def train_model(model, train_dataset, valid_dataset, num_epochs):\n",
        "    # 检查 CUDA 是否可用，并将模型移至 GPU\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    # 定义优化器和损失函数\n",
        "    criterion_token = nn.MSELoss()\n",
        "    criterion_label = nn.BCEWithLogitsLoss(reduction='none')\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "\n",
        "    best_roc_auc = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # 设置模型为训练模式\n",
        "        total_loss = 0\n",
        "        from tqdm import tqdm\n",
        "        for features, label in tqdm(train_dataset):\n",
        "            features = features.cuda() if torch.cuda.is_available() else features\n",
        "            label = label.cuda() if torch.cuda.is_available() else label\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            token_change_pred, label_pred = model(features.unsqueeze(0))\n",
        "\n",
        "            actual_token_change = features[1:, :] - features[:-1, :]\n",
        "            loss_token = criterion_token(token_change_pred[:, :-1], actual_token_change.unsqueeze(0))\n",
        "            label = label.float().unsqueeze(0).unsqueeze(-1).expand_as(label_pred)\n",
        "            loss_label = criterion_label(label_pred, label).mean()\n",
        "            loss = loss_label * 10\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_dataset)}\")\n",
        "\n",
        "        # 验证步骤\n",
        "        current_roc_auc = evaluate_model(model, valid_dataset)\n",
        "        print(f\"Validation ROC-AUC for Epoch {epoch+1}: {current_roc_auc}\")\n",
        "\n",
        "        # 检查是否需要更新最佳模型\n",
        "        if current_roc_auc > best_roc_auc:\n",
        "            best_roc_auc = current_roc_auc\n",
        "            best_model = model.state_dict()\n",
        "\n",
        "    # 保存最佳模型\n",
        "    if best_model is not None:\n",
        "        torch.save(best_model, 'best_model.pth')\n",
        "        print(\"Best model saved.\")\n",
        "\n",
        "    return best_model\n",
        "\n",
        "# 使用\n",
        "num_epochs = 10\n",
        "best_model = train_model(model, train_dataset, valid_dataset, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zSyg7pBXwEq",
        "outputId": "852c6081-ab2b-4892-fc4a-8722b2cc07d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9271/9271 [00:32<00:00, 281.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 4.014509346726295\n",
            "Validation ROC-AUC for Epoch 1: 0.723106846867879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9271/9271 [00:32<00:00, 283.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 3.988595562393438\n",
            "Validation ROC-AUC for Epoch 2: 0.7203586599607184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9271/9271 [00:32<00:00, 286.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 3.9299675409779837\n",
            "Validation ROC-AUC for Epoch 3: 0.7233580661696415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9271/9271 [00:32<00:00, 283.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 3.8819679188719975\n",
            "Validation ROC-AUC for Epoch 4: 0.7268332665106907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9271/9271 [00:34<00:00, 268.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 3.7936494608756575\n",
            "Validation ROC-AUC for Epoch 5: 0.7007876613259305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9271/9271 [00:32<00:00, 283.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 3.733960345477467\n",
            "Validation ROC-AUC for Epoch 6: 0.7109366673602688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9271/9271 [00:32<00:00, 282.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Loss: 3.6771852045362468\n",
            "Validation ROC-AUC for Epoch 7: 0.6950920883682925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9271/9271 [00:32<00:00, 283.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Loss: 3.6213906680072747\n",
            "Validation ROC-AUC for Epoch 8: 0.7065783931099935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9271/9271 [00:32<00:00, 281.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Loss: 3.5868179209457223\n",
            "Validation ROC-AUC for Epoch 9: 0.6997320327447866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9271/9271 [00:32<00:00, 283.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 3.544289237001402\n",
            "Validation ROC-AUC for Epoch 10: 0.6977514603707896\n",
            "Best model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model_new(model, eval_dataset):\n",
        "    model.eval()  # 设置模型为评估模式\n",
        "\n",
        "    # 确保模型在 GPU 上\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():  # 在评估过程中不计算梯度\n",
        "        for features, label in eval_dataset:\n",
        "            # 将数据移至 GPU\n",
        "            features = features.cuda() if torch.cuda.is_available() else features\n",
        "            label = label.cuda() if torch.cuda.is_available() else label\n",
        "\n",
        "            # 前向传播\n",
        "            _, label_pred = model(features.unsqueeze(0))\n",
        "\n",
        "            # 将预测转换为二元概率（sigmoid 函数）\n",
        "            label_prob = torch.sigmoid(label_pred).squeeze().cpu().numpy()[-1]\n",
        "            predictions.append(label_prob)\n",
        "\n",
        "            # 保存真实标签\n",
        "            true_labels.append(label.cpu().numpy())\n",
        "\n",
        "    # 计算 ROC-AUC 分数\n",
        "    predictions = (predictions+y_score_gnn)/2\n",
        "    roc_auc = roc_auc_score(true_labels, predictions)\n",
        "    return roc_auc\n",
        "\n",
        "# 使用\n",
        "# valid_dataset 应该是一个适当的 DataLoader\n",
        "roc_auc_score_new = evaluate_model_new(model, valid_dataset)\n",
        "print(f\"ROC-AUC Score: {roc_auc_score_new}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouHPZOwSwcyX",
        "outputId": "20a58433-9ad9-49e8-c209-f7028338312f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.7344713483117555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_score_gnn[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-_M0itAw4fx",
        "outputId": "33540b98-4c82-4644-cd36-342c7229287b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.15961881, 0.03937966, 0.2569339 , 0.40570876, 0.03302091,\n",
              "       0.19394471, 0.7560153 , 0.2331564 , 0.6652355 , 0.18563902],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# Setting the device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transform class\n",
        "class Transform(nn.Module):\n",
        "    def __init__(self, outfea, d):\n",
        "        super(Transform, self).__init__()\n",
        "        self.qff = nn.Linear(outfea, outfea)\n",
        "        self.kff = nn.Linear(outfea, outfea)\n",
        "        self.vff = nn.Linear(outfea, outfea)\n",
        "\n",
        "        self.ln = nn.LayerNorm(outfea)\n",
        "        self.lnff = nn.LayerNorm(outfea)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(outfea, outfea),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(outfea, outfea)\n",
        "        )\n",
        "\n",
        "        self.d = d\n",
        "\n",
        "    def forward(self, x):\n",
        "        query = self.qff(x)\n",
        "        key = self.kff(x)\n",
        "        value = self.vff(x)\n",
        "\n",
        "        query = torch.cat(torch.split(query, self.d, -1), 0).permute(0,2,1,3)\n",
        "        key = torch.cat(torch.split(key, self.d, -1), 0).permute(0,2,3,1)\n",
        "        value = torch.cat(torch.split(value, self.d, -1), 0).permute(0,2,1,3)\n",
        "\n",
        "        A = torch.matmul(query, key)\n",
        "        A /= (self.d ** 0.5)\n",
        "        A = torch.softmax(A, -1)\n",
        "\n",
        "        value = torch.matmul(A ,value)\n",
        "        value = torch.cat(torch.split(value, x.shape[0], 0), -1).permute(0,2,1,3)\n",
        "        value += x\n",
        "\n",
        "        value = self.ln(value)\n",
        "        x = self.ff(value) + value\n",
        "        return self.lnff(x)\n",
        "\n",
        "# PositionalEncoding class\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, outfea):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.outfea = outfea if outfea % 2 == 0 else outfea + 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        max_len = x.size(1)\n",
        "        pe = torch.zeros(max_len, self.outfea).to(x.device)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, self.outfea, 2).float() * -(math.log(10000.0) / self.outfea))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).unsqueeze(2)  # [1, T, 1, F]\n",
        "        x = x + pe[:,:,:,:x.size(-1)]  # Adjust positional encoding size to match input size\n",
        "        return x\n",
        "\n",
        "# SGNN class\n",
        "class SGNN(nn.Module):\n",
        "    def __init__(self, outfea):\n",
        "        super(SGNN, self).__init__()\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(outfea, outfea),\n",
        "            nn.Linear(outfea, outfea)\n",
        "        )\n",
        "        self.ff1 = nn.Linear(outfea, outfea)\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.ff(x)\n",
        "        a = torch.matmul(p, p.transpose(-1,-2))\n",
        "        R = torch.relu(torch.softmax(a, -1)) + torch.eye(x.shape[1]).to(device)\n",
        "\n",
        "        D = (R.sum(-1) ** -0.5)\n",
        "        D[torch.isinf(D)] = 0.\n",
        "        D = torch.diag_embed(D)\n",
        "\n",
        "        A = torch.matmul(torch.matmul(D, R), D)\n",
        "        x = torch.relu(self.ff1(torch.matmul(A, x)))\n",
        "        return x\n",
        "\n",
        "# GRU class\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, outfea):\n",
        "        super(GRU, self).__init__()\n",
        "        self.ff = nn.Linear(2*outfea, 2*outfea)\n",
        "        self.zff = nn.Linear(2*outfea, outfea)\n",
        "        self.outfea = outfea\n",
        "\n",
        "    def forward(self, x, xh):\n",
        "        r, u = torch.split(torch.sigmoid(self.ff(torch.cat([x, xh], -1))), self.outfea, -1)\n",
        "        z = torch.tanh(self.zff(torch.cat([x, r*xh], -1)))\n",
        "        x = u * z + (1-u) * xh\n",
        "        return x\n",
        "\n",
        "\n",
        "class STGNNwithGRU(nn.Module):\n",
        "    def __init__(self, outfea):\n",
        "        super(STGNNwithGRU, self).__init__()\n",
        "        self.sgnn = SGNN(outfea)  # 使用单个SGNN实例\n",
        "        self.gru = GRU(outfea)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, N, F = x.shape\n",
        "        hidden_state = torch.zeros([B, N, F]).to(device)\n",
        "        output = []\n",
        "\n",
        "        for i in range(T):\n",
        "            gx = self.sgnn(x[:, i, :, :])  # 在每个时间步使用相同的SGNN实例\n",
        "            gh = hidden_state\n",
        "            if i != 0:\n",
        "                gh = self.sgnn(hidden_state)  # 重复使用SGNN实例\n",
        "            hidden_state = self.gru(gx, gh)\n",
        "            output.append(hidden_state)\n",
        "\n",
        "        output = torch.stack(output, 1)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Continuing with the STGNNwithGRU class\n",
        "class STGNNwithGRU(nn.Module):\n",
        "    def __init__(self, outfea):\n",
        "        super(STGNNwithGRU, self).__init__()\n",
        "        self.sgnn = SGNN(outfea)  # Using a single SGNN instance\n",
        "        self.gru = GRU(outfea)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, N, F = x.shape\n",
        "        hidden_state = torch.zeros([B, N, F]).to(device)\n",
        "        output = []\n",
        "\n",
        "        for i in range(T):\n",
        "            gx = self.sgnn(x[:, i, :, :])  # Use the same SGNN instance at each time step\n",
        "            gh = hidden_state\n",
        "            if i != 0:\n",
        "                gh = self.sgnn(hidden_state)  # Reuse the SGNN instance\n",
        "            hidden_state = self.gru(gx, gh)\n",
        "            output.append(hidden_state)\n",
        "\n",
        "        output = torch.stack(output, 1)\n",
        "        return output\n",
        "\n",
        "# STGNN class\n",
        "class STGNN(nn.Module):\n",
        "    def __init__(self, infea, outfea, L, d):\n",
        "        super(STGNN, self).__init__()\n",
        "        self.start_emb = nn.Linear(infea, outfea)\n",
        "        self.adjust_emb = nn.Linear(outfea, infea)  # Additional layer for dimension adjustment\n",
        "        self.end_emb = nn.Linear(infea, infea)\n",
        "\n",
        "        self.stgnnwithgru = nn.ModuleList([STGNNwithGRU(outfea) for i in range(L)])\n",
        "        self.positional_encoding = PositionalEncoding(outfea)\n",
        "        self.transform = nn.ModuleList([Transform(outfea, d) for i in range(L)])\n",
        "\n",
        "        self.fc_token_change = nn.Linear(infea, infea)  # Predicts next time step change\n",
        "        self.fc_label = nn.Linear(infea, 1)  # Predicts label\n",
        "\n",
        "        self.L = L\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, F = x.shape\n",
        "        x = x.unsqueeze(2)  # Add spatial dimension [B, T, 1, F]\n",
        "        x = self.start_emb(x)\n",
        "        for i in range(self.L):\n",
        "            x = self.stgnnwithgru[i](x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for i in range(self.L):\n",
        "            x = self.transform[i](x)\n",
        "        x = self.adjust_emb(x)  # Adjust dimensions\n",
        "        x = self.end_emb(x)\n",
        "        x = x.squeeze(2)  # Remove spatial dimension [B, T, F]\n",
        "\n",
        "        token_change = self.fc_token_change(x)\n",
        "        labels = self.fc_label(x).squeeze(-1)\n",
        "\n",
        "        return token_change, labels\n",
        "\n",
        "# Instantiate and test the model\n",
        "model = STGNN(infea=171, outfea=64, L=3, d=8).to(device)\n",
        "test_input = torch.rand(1, 13, 171).to(device)\n",
        "token_change, labels = model(test_input)\n",
        "\n",
        "# Check the shapes of the outputs\n",
        "token_change.shape, labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckvobRgzxHwc",
        "outputId": "d9d673a5-eef7-4b33-a515-fc679804b2b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 13, 171]), torch.Size([1, 13]))"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "best_model = train_model(model, train_dataset, valid_dataset, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "BMILGjfK-ehQ",
        "outputId": "2b0b0843-c5d4-4c53-90f6-89e7855dca87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-6c252d3cc195>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-101-e42e5e7dd4cc>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataset, valid_dataset, num_epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mtoken_change_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mactual_token_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-114-64e8eaddd855>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstgnnwithgru\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-114-64e8eaddd855>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mgh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0mgh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reuse the SGNN instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-114-64e8eaddd855>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1680\u001b[0m     \u001b[0;31m# See full discussion on the problems with returning `Union` here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0;31m# https://github.com/microsoft/pyright/issues/4213\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fusion all pipeline"
      ],
      "metadata": {
        "id": "r3JKdKLo3PQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def extract_features_and_labels(dataset, feature_indices=None, retain_TS=False):\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for features, label in dataset:\n",
        "        # 如果提供了特征索引，只选择这些特征\n",
        "        if feature_indices is not None:\n",
        "            selected_features = features[:, feature_indices]\n",
        "        else:\n",
        "            selected_features = features\n",
        "\n",
        "        # 检查是否保留时间序列\n",
        "        if retain_TS:\n",
        "            # 保留时间序列的第一个维度\n",
        "            if selected_features.shape[0] < 30:\n",
        "                # 如果少于30天，使用最后一天的值进行填充\n",
        "                padding = torch.ones((30 - selected_features.shape[0], *selected_features.shape[1:])) * selected_features[-1]\n",
        "                selected_features = torch.cat([selected_features, padding], dim=0)\n",
        "            else:\n",
        "                # 只取最后30天的值\n",
        "                selected_features = selected_features[-30:]\n",
        "        else:\n",
        "            # 对时间维度进行平均\n",
        "            selected_features = selected_features.mean(dim=0)  # 对时间维度取平均值\n",
        "\n",
        "            # 添加住院天数作为一个新的特征\n",
        "            num_days = features.shape[0]  # 时间维度的大小就是住院天数\n",
        "            selected_features = torch.cat([selected_features, torch.tensor([num_days], dtype=torch.float32)])\n",
        "\n",
        "        features_list.append(selected_features)  # 存储特征\n",
        "        labels_list.append(label)  # 存储标签\n",
        "\n",
        "    # 将列表转换为Tensor\n",
        "    features_tensor = torch.stack(features_list)\n",
        "    labels_tensor = torch.stack(labels_list)\n",
        "\n",
        "    return features_tensor, labels_tensor"
      ],
      "metadata": {
        "id": "xaKBjxvr_lnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title indice\n",
        "demo_indices = list(range(3))  # demo 特征在前3个位置\n",
        "\n",
        "icd_cols_indices = list(range(3, 3+91))  # icd_cols 特征在接下来的91个位置\n",
        "\n",
        "\n",
        "lab_cols_indices = list(range(3+91, 3+91+36))  # lab_cols 特征在接下来的36个位置\n",
        "#加一个time feature\n",
        "\n",
        "med_cols_indices = list(range(3+91+36, 3+91+36+41))  # med_cols 特征在接下来的41个位置\n",
        "#加一个feature为这些value的standarlized sum（？）\n",
        "\n",
        "# 打印结果\n",
        "print(\"demo_indices:\", demo_indices)\n",
        "print(\"icd_cols_indices:\", icd_cols_indices)\n",
        "print(\"lab_cols_indices:\", lab_cols_indices)\n",
        "print(\"med_cols_indices:\", med_cols_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJc-0HN7IiGF",
        "outputId": "7f0ebf83-6926-4262-dc4f-7cf665c1057b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "demo_indices: [0, 1, 2]\n",
            "icd_cols_indices: [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93]\n",
            "lab_cols_indices: [94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]\n",
            "med_cols_indices: [130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 三个model\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=3, dropout_rate=0.8):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # 定义三层LSTM，每层之间添加dropout\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
        "                            batch_first=True, dropout=dropout_rate)\n",
        "        # Dropout层\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # 定义线性层\n",
        "        self.fc_token = nn.Linear(hidden_size, output_size)  # 用于 token 改变量的预测\n",
        "        self.fc_label = nn.Linear(hidden_size, 3)  # 用于每个时间步的标签预测\n",
        "\n",
        "    def forward(self, x):\n",
        "        # LSTM 层\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        # 应用dropout\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        # 在每个时间步预测 token 的改变量\n",
        "        token_change = self.fc_token(lstm_out)\n",
        "        # 在每个时间步预测标签\n",
        "        labels = self.fc_label(lstm_out)\n",
        "        return token_change, labels\n",
        "\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        #self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc(x)\n",
        "        #out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "class SimpleFusionNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, dropout_rate=0.5):\n",
        "        super(SimpleFusionNetwork, self).__init__()\n",
        "\n",
        "        # Input layer\n",
        "        self.input_layer = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid() #因为pred是prob所以框在0-1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "class VerySimpleFusionNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "      super(VerySimpleFusionNetwork, self).__init__()\n",
        "      self.fc = nn.Linear(input_dim, output_dim)\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc(x)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FusionModel(nn.Module):\n",
        "    def __init__(self, lstm_input_size, lstm_hidden_size, lstm_output_size, nn_input_dim, nn_output_dim, fusion_input_dim, fusion_hidden_dim, dropout_rate=0.5):\n",
        "        super(FusionModel, self).__init__()\n",
        "\n",
        "        # 定义LSTM模型\n",
        "        self.lstm_model = LSTMModel(lstm_input_size, lstm_hidden_size, lstm_output_size, dropout_rate=dropout_rate)\n",
        "\n",
        "        # 定义逻辑回归模型\n",
        "        self.nn_model = LogisticRegression(nn_input_dim, nn_output_dim)\n",
        "\n",
        "        # 定义融合网络\n",
        "        self.fusion_model = SimpleFusionNetwork(fusion_input_dim, fusion_hidden_dim, dropout_rate=dropout_rate)\n",
        "\n",
        "    def forward(self, batch_features_1, batch_features_2):\n",
        "        _, lstm_out = self.lstm_model(batch_features_2)\n",
        "        nn_out = self.nn_model(batch_features_1)\n",
        "\n",
        "        # 取 LSTM 输出的最后一个时间步\n",
        "        lstm_out_last = lstm_out[:, -1, :]\n",
        "\n",
        "        # 融合 LSTM 和 NN 的输出\n",
        "        fusion_in = torch.cat((nn_out, lstm_out_last), dim=1)\n",
        "        fusion_out = self.fusion_model(fusion_in)\n",
        "\n",
        "        return fusion_out"
      ],
      "metadata": {
        "id": "afs-e6OwPcb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialze model + dataloader\n",
        "nn_model = LogisticRegression(input_dim=136,output_dim=1)\n",
        "lstm_model = LSTMModel(input_size=36, hidden_size=128, output_size=50)\n",
        "fusion_model = VerySimpleFusionNetwork(input_dim=2, output_dim=1)\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_features_1 = torch.tensor(train_features_1, dtype=torch.float32)\n",
        "train_features_2 = torch.tensor(train_features_2, dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.float32)\n",
        "\n",
        "test_features_1 = torch.tensor(test_features_1, dtype=torch.float32)\n",
        "test_features_2 = torch.tensor(test_features_2, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test_labels, dtype=torch.float32)\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(train_features_1, train_features_2, train_labels)\n",
        "test_dataset = TensorDataset(test_features_1, test_features_2, test_labels)\n",
        "\n",
        "#dataloader\n",
        "batch_size = 1000\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOR0oanbRVZK",
        "outputId": "1a060238-1825-48a0-d091-f2f2ee2d59ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-5e9fe672406f>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_features_1 = torch.tensor(train_features_1, dtype=torch.float32)\n",
            "<ipython-input-79-5e9fe672406f>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_features_2 = torch.tensor(train_features_2, dtype=torch.float32)\n",
            "<ipython-input-79-5e9fe672406f>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_labels = torch.tensor(train_labels, dtype=torch.float32)\n",
            "<ipython-input-79-5e9fe672406f>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_features_1 = torch.tensor(test_features_1, dtype=torch.float32)\n",
            "<ipython-input-79-5e9fe672406f>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_features_2 = torch.tensor(test_features_2, dtype=torch.float32)\n",
            "<ipython-input-79-5e9fe672406f>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_labels = torch.tensor(test_labels, dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_indices = list(range(3))  # demo 特征在前3个位置\n",
        "\n",
        "icd_cols_indices = list(range(3, 3+91))  # icd_cols 特征在接下来的91个位置\n",
        "\n",
        "\n",
        "lab_cols_indices = list(range(3+91, 3+91+36))  # lab_cols 特征在接下来的36个位置\n",
        "#加一个time feature\n",
        "\n",
        "med_cols_indices = list(range(3+91+36, 3+91+36+41))\n",
        "\n",
        "\n",
        "print(\n",
        "    data['feat_dict']['11674366_29673314'][1][3:3+91]\n",
        ")\n",
        "print(data['feat_dict']['11674366_29673314'][1][3:3+91])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyorhozBsJ6i",
        "outputId": "fd0a4ae6-6131-4759-b514-8523e3db183a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train and evaluate: part 1\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch.optim as optim\n",
        "\n",
        "input_dim = 172  # 这是一个示例值，你需要根据你的数据来设置\n",
        "hidden_dim = 10  # 这也是一个示例值，你可以根据你的需求来设置\n",
        "output_dim = 1  # 这是一个例子，假设你的问题是二分类问题\n",
        "\n",
        "nn_model = LogisticRegression(input_dim, output_dim)\n",
        "\n",
        "num_epochs=10\n",
        "learning_rate=1e-4\n",
        "\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = optim.SGD(list(nn_model.parameters()), lr=learning_rate)  # 只优化 nn_model 的参数\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_features_1, _, batch_labels in test_dataloader:  # 只使用 feature1 和 labels\n",
        "\n",
        "        nn_out = nn_model(batch_features_1)\n",
        "\n",
        "        # 计算loss\n",
        "        batch_labels = batch_labels.unsqueeze(1)\n",
        "        loss = criterion(nn_out, batch_labels)  # 直接使用 nn_out 计算 loss\n",
        "\n",
        "        # 反向传播和优化\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(f'Loss: {loss.item()}')\n",
        "\n",
        "    #一个epoch结束\n",
        "    nn_model.eval()\n",
        "\n",
        "    preds = []\n",
        "    true_labels = []\n",
        "\n",
        "    #infer+evaluate\n",
        "    with torch.no_grad():\n",
        "        for batch_features_1, _,batch_labels in test_dataloader:  # 只使用 feature1 和 labels\n",
        "            nn_out = nn_model(batch_features_1)\n",
        "\n",
        "            probas = nn_out.squeeze().cpu().numpy()\n",
        "            preds.extend(probas)\n",
        "            true_labels.extend(batch_labels.squeeze().cpu().numpy())\n",
        "\n",
        "    # 计算本次epoch的AUC\n",
        "    auc = roc_auc_score(true_labels, preds)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Test AUC: {auc}')\n",
        "\n",
        "    #训练模式\n",
        "    nn_model.train()"
      ],
      "metadata": {
        "id": "rdOCTfXzxl38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实例化模型\n",
        "input_dim = 100  # 例如，你的输入维度是100\n",
        "output_dim = 1  # 例如，你的输出维度是1\n",
        "model = LogisticRegression(input_dim, output_dim)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.BCEWithLogitsLoss()  # 由于你没有在模型中使用sigmoid，所以这里使用BCEWithLogitsLoss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # 使用SGD优化器\n",
        "\n",
        "# 训练模型\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_features, batch_labels in train_dataloader:\n",
        "        # 前向传播\n",
        "        outputs = model(batch_features)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "\n",
        "        # 反向传播和优化\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # 打印损失信息\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "vSncfCoLpJwX",
        "outputId": "d26ed1a0-ce7e-4bb2-97de-f46cdc80298c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-f964e2456959>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# 前向传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_7rJrx3Xypl",
        "outputId": "dcb10c50-4d3f-4d2b-921a-9477920b6e18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}